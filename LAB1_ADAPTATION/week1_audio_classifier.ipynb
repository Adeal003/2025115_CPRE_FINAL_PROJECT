{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1: Audio Classification w/ IRMAS Dataset - TinyImageNet Workflow for Audio Spectrogram Classification\n",
    "## Tools Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.20.0\n",
      "GPU Available: []\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import os\n",
    "import sys\n",
    "import operator\n",
    "import re\n",
    "import datetime\n",
    "from functools import reduce\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D, ReLU, Softmax\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "# Enable or disable GPU\n",
    "ENABLE_GPU = True\n",
    "if not ENABLE_GPU:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section One: Dataset Acquisition & Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected Instrument Classes (10):\n",
      "  0: cel\n",
      "  1: cla\n",
      "  2: flu\n",
      "  3: gac\n",
      "  4: gel\n",
      "  5: org\n",
      "  6: pia\n",
      "  7: sax\n",
      "  8: tru\n",
      "  9: vio\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "IRMAS Dataset Structure:\n",
    "- Downloaded from: https://www.upf.edu/web/mtg/irmas\n",
    "- Contains audio files of 11 instrument classes\n",
    "- Training: 6705 files (3 second excerpts)\n",
    "- Testing: 2874 files (variable length)\n",
    "\n",
    "For this project, we'll subset to 5-10 instrument classes.\n",
    "\"\"\"\n",
    "\n",
    "# TODO: Download IRMAS dataset and update this path\n",
    "IRMAS_PATH = os.path.expanduser(\"~/irmas_dataset\")\n",
    "TRAIN_PATH = os.path.join(IRMAS_PATH, \"IRMAS-TrainingData\")\n",
    "TEST_PATH = os.path.join(IRMAS_PATH, \"IRMAS-TestingData\")\n",
    "\n",
    "# Define instrument classes (subset to keep manageable)\n",
    "# Full IRMAS classes: cel, cla, flu, gac, gel, org, pia, sax, tru, vio, voi\n",
    "INSTRUMENT_CLASSES = ['cel', 'cla', 'flu', 'gac', 'gel', 'org', 'pia', 'sax', 'tru', 'vio']\n",
    "NUM_CLASSES = len(INSTRUMENT_CLASSES)\n",
    "\n",
    "print(f\"\\nSelected Instrument Classes ({NUM_CLASSES}):\")\n",
    "for i, inst in enumerate(INSTRUMENT_CLASSES):\n",
    "    print(f\"  {i}: {inst}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Statistics Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Exploring IRMAS Training Dataset...\n"
     ]
    }
   ],
   "source": [
    "def explore_irmas_dataset(data_path, instrument_classes):\n",
    "    \"\"\"\n",
    "    Explore IRMAS dataset structure and collect statistics\n",
    "    \"\"\"\n",
    "    stats = {\n",
    "        'instrument': [],\n",
    "        'num_files': [],\n",
    "        'avg_duration': [],\n",
    "        'sample_rate': [],\n",
    "        'total_samples': []\n",
    "    }\n",
    "    \n",
    "    for inst in instrument_classes:\n",
    "        inst_path = os.path.join(data_path, inst)\n",
    "        if not os.path.exists(inst_path):\n",
    "            print(f\"Warning: {inst_path} not found!\")\n",
    "            continue\n",
    "            \n",
    "        audio_files = [f for f in os.listdir(inst_path) if f.endswith('.wav')]\n",
    "        num_files = len(audio_files)\n",
    "        \n",
    "        # Sample a few files to get duration statistics\n",
    "        durations = []\n",
    "        sample_rates = []\n",
    "        for audio_file in audio_files[:10]:  # Sample first 10 files\n",
    "            file_path = os.path.join(inst_path, audio_file)\n",
    "            try:\n",
    "                y, sr = librosa.load(file_path, sr=None)\n",
    "                durations.append(len(y) / sr)\n",
    "                sample_rates.append(sr)\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        avg_duration = np.mean(durations) if durations else 0\n",
    "        common_sr = max(set(sample_rates), key=sample_rates.count) if sample_rates else 0\n",
    "        \n",
    "        stats['instrument'].append(inst)\n",
    "        stats['num_files'].append(num_files)\n",
    "        stats['avg_duration'].append(avg_duration)\n",
    "        stats['sample_rate'].append(common_sr)\n",
    "        stats['total_samples'].append(num_files)\n",
    "    \n",
    "    return pd.DataFrame(stats)\n",
    "\n",
    "# TODO: Explore the IRMAS dataset\n",
    "print(\"\\nExploring IRMAS Training Dataset...\")\n",
    "# dataset_stats = explore_irmas_dataset(TRAIN_PATH, INSTRUMENT_CLASSES)\n",
    "# print(\"\\nDataset Statistics:\")\n",
    "# print(dataset_stats)\n",
    "# print(f\"\\nTotal audio files: {dataset_stats['num_files'].sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 2: AUDIO PREPROCESSING PIPELINE (Days 3-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spectrogram Parameters\n",
    "SAMPLE_RATE = 22050  # Standard audio sample rate\n",
    "N_FFT = 2048\n",
    "HOP_LENGTH = 512\n",
    "N_MELS = 128  # Number of mel bands\n",
    "DURATION = 3.0  # Fixed duration in seconds\n",
    "TARGET_SHAPE = (128, 128, 1)  # Match CNN input expectations (height, width, channels)\n",
    "\n",
    "def audio_to_melspectrogram(audio_path, sr=SAMPLE_RATE, n_mels=N_MELS, \n",
    "                           duration=DURATION, target_shape=TARGET_SHAPE):\n",
    "    \"\"\"\n",
    "    Convert audio file to mel-spectrogram\n",
    "    \n",
    "    Args:\n",
    "        audio_path: Path to audio file\n",
    "        sr: Sample rate\n",
    "        n_mels: Number of mel frequency bands\n",
    "        duration: Target duration in seconds\n",
    "        target_shape: Output shape (height, width, channels)\n",
    "    \n",
    "    Returns:\n",
    "        Mel-spectrogram as numpy array with shape target_shape\n",
    "    \"\"\"\n",
    "    # Load audio file\n",
    "    y, sr_actual = librosa.load(audio_path, sr=sr, duration=duration)\n",
    "    \n",
    "    # Pad if too short\n",
    "    target_length = int(sr * duration)\n",
    "    if len(y) < target_length:\n",
    "        y = np.pad(y, (0, target_length - len(y)), mode='constant')\n",
    "    \n",
    "    # Generate mel-spectrogram\n",
    "    mel_spec = librosa.feature.melspectrogram(\n",
    "        y=y, \n",
    "        sr=sr, \n",
    "        n_fft=N_FFT,\n",
    "        hop_length=HOP_LENGTH,\n",
    "        n_mels=n_mels\n",
    "    )\n",
    "    \n",
    "    # Convert to log scale (dB)\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    \n",
    "    # Normalize to [0, 1]\n",
    "    mel_spec_norm = (mel_spec_db - mel_spec_db.min()) / (mel_spec_db.max() - mel_spec_db.min())\n",
    "    \n",
    "    # Resize to target shape if needed\n",
    "    if mel_spec_norm.shape != target_shape[:2]:\n",
    "        from scipy.ndimage import zoom\n",
    "        zoom_factors = (target_shape[0] / mel_spec_norm.shape[0], \n",
    "                       target_shape[1] / mel_spec_norm.shape[1])\n",
    "        mel_spec_norm = zoom(mel_spec_norm, zoom_factors, order=1)\n",
    "    \n",
    "    # Add channel dimension\n",
    "    mel_spec_norm = np.expand_dims(mel_spec_norm, axis=-1)\n",
    "    \n",
    "    return mel_spec_norm.astype(np.float32)\n",
    "\n",
    "def visualize_spectrograms(audio_paths, labels, num_samples=5):\n",
    "    \"\"\"\n",
    "    Visualize sample spectrograms\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, num_samples, figsize=(15, 6))\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Load audio\n",
    "        y, sr = librosa.load(audio_paths[i], sr=SAMPLE_RATE, duration=DURATION)\n",
    "        \n",
    "        # Plot waveform\n",
    "        axes[0, i].plot(y)\n",
    "        axes[0, i].set_title(f\"{labels[i]}\\nWaveform\")\n",
    "        axes[0, i].set_xlabel(\"Sample\")\n",
    "        \n",
    "        # Plot spectrogram\n",
    "        mel_spec = audio_to_melspectrogram(audio_paths[i])\n",
    "        axes[1, i].imshow(mel_spec[:, :, 0], aspect='auto', origin='lower', cmap='viridis')\n",
    "        axes[1, i].set_title(\"Mel-Spectrogram\")\n",
    "        axes[1, i].set_xlabel(\"Time\")\n",
    "        axes[1, i].set_ylabel(\"Mel Frequency\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('sample_spectrograms.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# TODO: Generate and visualize sample spectrograms\n",
    "# Collect sample audio files\n",
    "# sample_files = []\n",
    "# sample_labels = []\n",
    "# for inst in INSTRUMENT_CLASSES[:5]:\n",
    "#     inst_path = os.path.join(TRAIN_PATH, inst)\n",
    "#     files = [os.path.join(inst_path, f) for f in os.listdir(inst_path) if f.endswith('.wav')][:1]\n",
    "#     sample_files.extend(files)\n",
    "#     sample_labels.extend([inst] * len(files))\n",
    "#\n",
    "# visualize_spectrograms(sample_files, sample_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generator for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_audio_dataset(data_path, instrument_classes, batch_size=32, \n",
    "                        validation_split=0.2, shuffle=True):\n",
    "    \"\"\"\n",
    "    Create TensorFlow dataset from audio files\n",
    "    \"\"\"\n",
    "    all_files = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for idx, inst in enumerate(instrument_classes):\n",
    "        inst_path = os.path.join(data_path, inst)\n",
    "        if not os.path.exists(inst_path):\n",
    "            continue\n",
    "        \n",
    "        audio_files = [os.path.join(inst_path, f) \n",
    "                      for f in os.listdir(inst_path) if f.endswith('.wav')]\n",
    "        all_files.extend(audio_files)\n",
    "        all_labels.extend([idx] * len(audio_files))\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    all_files = np.array(all_files)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    # Shuffle\n",
    "    if shuffle:\n",
    "        indices = np.random.permutation(len(all_files))\n",
    "        all_files = all_files[indices]\n",
    "        all_labels = all_labels[indices]\n",
    "    \n",
    "    # Split into train and validation\n",
    "    split_idx = int(len(all_files) * (1 - validation_split))\n",
    "    train_files, val_files = all_files[:split_idx], all_files[split_idx:]\n",
    "    train_labels, val_labels = all_labels[:split_idx], all_labels[split_idx:]\n",
    "    \n",
    "    print(f\"Training samples: {len(train_files)}\")\n",
    "    print(f\"Validation samples: {len(val_files)}\")\n",
    "    \n",
    "    return (train_files, train_labels), (val_files, val_labels)\n",
    "\n",
    "def data_generator(files, labels, batch_size=32, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generator function for loading and preprocessing audio on-the-fly\n",
    "    \"\"\"\n",
    "    num_samples = len(files)\n",
    "    indices = np.arange(num_samples)\n",
    "    \n",
    "    while True:\n",
    "        if shuffle:\n",
    "            np.random.shuffle(indices)\n",
    "        \n",
    "        for start_idx in range(0, num_samples, batch_size):\n",
    "            batch_indices = indices[start_idx:start_idx + batch_size]\n",
    "            batch_files = files[batch_indices]\n",
    "            batch_labels = labels[batch_indices]\n",
    "            \n",
    "            # Load and preprocess audio\n",
    "            batch_spectrograms = []\n",
    "            for audio_file in batch_files:\n",
    "                try:\n",
    "                    spec = audio_to_melspectrogram(audio_file)\n",
    "                    batch_spectrograms.append(spec)\n",
    "                except Exception as e:\n",
    "                    # Skip problematic files\n",
    "                    print(f\"Error loading {audio_file}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            if not batch_spectrograms:\n",
    "                continue\n",
    "            \n",
    "            X = np.array(batch_spectrograms)\n",
    "            y = tf.keras.utils.to_categorical(\n",
    "                batch_labels[:len(batch_spectrograms)], \n",
    "                num_classes=NUM_CLASSES\n",
    "            )\n",
    "            \n",
    "            yield X, y\n",
    "\n",
    "# TODO: Create train/validation splits\n",
    "# (train_files, train_labels), (val_files, val_labels) = create_audio_dataset(\n",
    "#     TRAIN_PATH, \n",
    "#     INSTRUMENT_CLASSES,\n",
    "#     validation_split=0.2\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 3: MODEL ARCHITECTURE (Days 5-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_audio_cnn(input_shape=(128, 128, 1), num_classes=NUM_CLASSES):\n",
    "    \"\"\"\n",
    "    CNN architecture adapted from TinyImageNet model for audio spectrograms\n",
    "    \n",
    "    Original architecture adapted for single-channel spectrogram input\n",
    "    \"\"\"\n",
    "    model = Sequential(name='AudioCNN_IRMAS')\n",
    "    \n",
    "    # First conv block\n",
    "    model.add(Conv2D(32, (5, 5), input_shape=input_shape, activation='relu', name='conv1_1'))\n",
    "    model.add(Conv2D(32, (5, 5), activation='relu', name='conv1_2'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), name='pool1'))\n",
    "    \n",
    "    # Second conv block\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', name='conv2_1'))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', name='conv2_2'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), name='pool2'))\n",
    "    \n",
    "    # Third conv block\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', name='conv3_1'))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', name='conv3_2'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), name='pool3'))\n",
    "    \n",
    "    # Flatten and dense layers\n",
    "    model.add(Flatten(name='flatten'))\n",
    "    model.add(Dense(256, activation='relu', name='fc1'))\n",
    "    model.add(Dropout(0.5, name='dropout'))  # Added dropout for regularization\n",
    "    model.add(Dense(num_classes, activation='softmax', name='fc2'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create model\n",
    "audio_model = create_audio_cnn()\n",
    "audio_model.summary()\n",
    "\n",
    "# Compile model\n",
    "audio_model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    metrics=['accuracy', 'top_k_categorical_accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 4: MODEL TRAINING (Days 5-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 30\n",
    "STEPS_PER_EPOCH = 100  # Adjust based on dataset size\n",
    "VALIDATION_STEPS = 20\n",
    "\n",
    "# Setup TensorBoard logging\n",
    "log_dir = os.path.join(\"logs\", \"fit\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = TensorBoard(\n",
    "    log_dir=log_dir,\n",
    "    histogram_freq=1,\n",
    "    profile_batch='10,20'\n",
    ")\n",
    "\n",
    "# TODO: Train the model\n",
    "# Note: Uncomment and run this section when ready to train\n",
    "\"\"\"\n",
    "train_gen = data_generator(train_files, train_labels, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_gen = data_generator(val_files, val_labels, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "history = audio_model.fit(\n",
    "    train_gen,\n",
    "    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_gen,\n",
    "    validation_steps=VALIDATION_STEPS,\n",
    "    callbacks=[tensorboard_callback],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "audio_model.save('audio_classifier_irmas.h5')\n",
    "print(\"Model saved as 'audio_classifier_irmas.h5'\")\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 5: TRAINING VISUALIZATION & ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    Plot training and validation accuracy/loss curves\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot accuracy\n",
    "    axes[0].plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    axes[0].plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "    axes[0].set_title('Model Accuracy')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Accuracy')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True)\n",
    "    \n",
    "    # Plot loss\n",
    "    axes[1].plot(history.history['loss'], label='Train Loss')\n",
    "    axes[1].plot(history.history['val_loss'], label='Val Loss')\n",
    "    axes[1].set_title('Model Loss')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Loss')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_curves.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# TODO: Plot training curves after training\n",
    "# plot_training_history(history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 6: MODEL EVALUATION & INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_files, test_labels, batch_size=32):\n",
    "    \"\"\"\n",
    "    Evaluate model on test set\n",
    "    \"\"\"\n",
    "    test_gen = data_generator(test_files, test_labels, batch_size=batch_size, shuffle=False)\n",
    "    steps = len(test_files) // batch_size\n",
    "    \n",
    "    results = model.evaluate(test_gen, steps=steps, verbose=1)\n",
    "    \n",
    "    print(f\"\\nTest Loss: {results[0]:.4f}\")\n",
    "    print(f\"Test Accuracy: {results[1]:.4f}\")\n",
    "    print(f\"Test Top-K Accuracy: {results[2]:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def predict_instrument(model, audio_path, instrument_classes):\n",
    "    \"\"\"\n",
    "    Predict instrument class for a single audio file\n",
    "    \"\"\"\n",
    "    # Preprocess audio\n",
    "    spec = audio_to_melspectrogram(audio_path)\n",
    "    spec_batch = np.expand_dims(spec, axis=0)\n",
    "    \n",
    "    # Predict\n",
    "    predictions = model.predict(spec_batch, verbose=0)\n",
    "    predicted_class = np.argmax(predictions[0])\n",
    "    confidence = predictions[0][predicted_class]\n",
    "    \n",
    "    print(f\"\\nPrediction for: {os.path.basename(audio_path)}\")\n",
    "    print(f\"Predicted Instrument: {instrument_classes[predicted_class]}\")\n",
    "    print(f\"Confidence: {confidence:.4f}\")\n",
    "    \n",
    "    # Show top 5 predictions\n",
    "    top_5_idx = np.argsort(predictions[0])[-5:][::-1]\n",
    "    print(\"\\nTop 5 Predictions:\")\n",
    "    for idx in top_5_idx:\n",
    "        print(f\"  {instrument_classes[idx]}: {predictions[0][idx]:.4f}\")\n",
    "    \n",
    "    return predicted_class, confidence\n",
    "\n",
    "# TODO: Run inference on sample audio files\n",
    "# Load trained model\n",
    "# audio_model = load_model('audio_classifier_irmas.h5')\n",
    "\n",
    "# Test on a few samples\n",
    "# for i in range(3):\n",
    "#     sample_file = val_files[i]\n",
    "#     true_label = val_labels[i]\n",
    "#     print(f\"\\nTrue Label: {INSTRUMENT_CLASSES[true_label]}\")\n",
    "#     predict_instrument(audio_model, sample_file, INSTRUMENT_CLASSES)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 7: EXPORT MODEL WEIGHTS & INTERMEDIATE OUTPUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_model_weights(model, output_dir='model_weights'):\n",
    "    \"\"\"\n",
    "    Export model weights and biases as binary files for C++ implementation\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for layer in model.layers:\n",
    "        if len(layer.get_weights()) > 0:\n",
    "            weights = layer.get_weights()\n",
    "            layer_name = layer.name\n",
    "            \n",
    "            # Export weights\n",
    "            if len(weights) > 0:\n",
    "                weight_array = weights[0]\n",
    "                weight_file = os.path.join(output_dir, f'{layer_name}_weights.bin')\n",
    "                weight_array.astype(np.float32).tofile(weight_file)\n",
    "                print(f\"Exported: {weight_file} | Shape: {weight_array.shape}\")\n",
    "            \n",
    "            # Export biases\n",
    "            if len(weights) > 1:\n",
    "                bias_array = weights[1]\n",
    "                bias_file = os.path.join(output_dir, f'{layer_name}_bias.bin')\n",
    "                bias_array.astype(np.float32).tofile(bias_file)\n",
    "                print(f\"Exported: {bias_file} | Shape: {bias_array.shape}\")\n",
    "\n",
    "def export_intermediate_features(model, audio_path, output_dir='feature_maps'):\n",
    "    \"\"\"\n",
    "    Export intermediate feature maps for validation\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Create intermediate models for each layer\n",
    "    spec = audio_to_melspectrogram(audio_path)\n",
    "    spec_batch = np.expand_dims(spec, axis=0)\n",
    "    \n",
    "    for i, layer in enumerate(model.layers):\n",
    "        intermediate_model = Model(inputs=model.input, outputs=layer.output)\n",
    "        features = intermediate_model.predict(spec_batch, verbose=0)\n",
    "        \n",
    "        # Save features\n",
    "        feature_file = os.path.join(output_dir, f'layer_{i}_{layer.name}_features.bin')\n",
    "        features.astype(np.float32).tofile(feature_file)\n",
    "        print(f\"Exported: {feature_file} | Shape: {features.shape}\")\n",
    "\n",
    "# TODO: Export weights and features after training\n",
    "# export_model_weights(audio_model)\n",
    "# export_intermediate_features(audio_model, val_files[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DELIVERABLES CHECKLIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"WEEK 1 DELIVERABLES CHECKLIST\")\n",
    "print(\"=\"*70)\n",
    "print(\"[ ] Dataset downloaded and explored\")\n",
    "print(\"[ ] Audio-to-spectrogram pipeline implemented\")\n",
    "print(\"[ ] Train/validation/test splits created\")\n",
    "print(\"[ ] Sample spectrograms visualized\")\n",
    "print(\"[ ] CNN model architecture adapted for audio\")\n",
    "print(\"[ ] Model trained on GPU VM\")\n",
    "print(\"[ ] Training curves documented\")\n",
    "print(\"[ ] Validation accuracy >= 70-80%\")\n",
    "print(\"[ ] Model saved as .h5 file\")\n",
    "print(\"[ ] Model weights exported as binary files\")\n",
    "print(\"[ ] TensorBoard profiling completed\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# To launch TensorBoard:\n",
    "# tensorboard --logdir=logs/fit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

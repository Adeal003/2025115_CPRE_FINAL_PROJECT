{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//import pathlib, os, sys, operator, re, datetime\n",
    "from functools import reduce\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from keras import Model\n",
    "import tensorflow_datasets as tfds\n",
    "from tiny_imagenet import TinyImagenetDataset\n",
    "\n",
    "# Enable or disable GPU\n",
    "# To ffully disable it, we need to hide all GPU devices from Tensorflow#NEED TO COMMIT\n",
    "# Make sure GPU is dikksabled for this inference part of the lab\n",
    "ENABLE_GPU = False\n",
    "# tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "if not ENABLE_GPU:\n",
    "    tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "# Print Python and TF version, and where we are running\n",
    "print(f'Running on Python Version: {sys.version}')\n",
    "print(f'Using Tensorflow Version: {tf. __version__}')\n",
    "if not tf.config.experimental.list_physical_devices(\"GPU\"):\n",
    "    print('Running on CPU')\n",
    "else:\n",
    "    print(f'Using GPU at: {tf.test.gpu_device_name()} (of {len(tf.config.experimental.list_physical_devices(\"GPU\"))} available)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell imports our dataset.\n",
    "\n",
    "# Original Source: https://github.com/ksachdeva/tiny-imagenet-tfds\n",
    "# Class Version Source: https://github.com/duweisu/tiny-imagenet-tfds\n",
    "# Setup our dataset\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "tiny_imagenet_builder = TinyImagenetDataset()\n",
    "\n",
    "# this call (download_and_prepare) will trigger the download of the dataset\n",
    "# and preparation (conversion to tfrecords)\n",
    "#\n",
    "# This will be done only once and on next usage tfds will\n",
    "# use the cached version on your host.\n",
    "tiny_imagenet_builder.download_and_prepare(download_dir=\"~/tensorflow-datasets/downloads\")\n",
    "\n",
    "# class_names = tiny_imagenet_builder.info.features['label'].names\n",
    "ds = tiny_imagenet_builder.as_dataset()\n",
    "ds_train, ds_val = ds[\"train\"], ds[\"validation\"]\n",
    "assert(isinstance(ds_train, tf.data.Dataset))\n",
    "assert(isinstance(ds_val, tf.data.Dataset))\n",
    "\n",
    "# Training Dataset\n",
    "ds_train = ds_train.shuffle(1024).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Validation Dataset\n",
    "ds_val = ds_val.shuffle(1024).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Dataset metadata\n",
    "ds_info = tiny_imagenet_builder.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to read the \"human readable\" labels so we can translate with the numeric values\n",
    "# Read the labels file (words.txt)\n",
    "with open(os.path.abspath('wnids.txt'), 'r') as f:\n",
    "    wnids = [x.strip() for x in f]\n",
    "\n",
    "# Map wnids to integer labels\n",
    "wnid_to_label = {wnid: i for i, wnid in enumerate(wnids)}\n",
    "label_to_wnid = {v: k for k, v in wnid_to_label.items()}\n",
    "\n",
    "# Use words.txt to get names for each class\n",
    "with open(os.path.abspath('words.txt'), 'r') as f:\n",
    "    wnid_to_words = dict(line.split('\\t') for line in f)\n",
    "    for wnid, words in wnid_to_words.items():\n",
    "        wnid_to_words[wnid] = [w.strip() for w in words.split(',')]\n",
    "        \n",
    "class_names = [str(wnid_to_words[wnid]) for wnid in wnids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to get the label name\n",
    "def img_class(img_data, idx=None):\n",
    "    image, label, id, label_name = img_data[\"image\"], img_data[\"label\"], img_data[\"id\"], img_data[\"metadata\"]['label_name']\n",
    "    # Handle batches of images correctly\n",
    "    if idx != None:\n",
    "        image, label, id, label_name = img_data[\"image\"][idx], img_data[\"label\"][idx], img_data[\"id\"][idx], img_data[\"metadata\"]['label_name'][idx]\n",
    "    \n",
    "    return f\"{label_name} (class index: {label} - id: {id})\"\n",
    "\n",
    "\n",
    "# Helper function to show basic info about an image\n",
    "def img_info(img, idx=None, display=True, title_apend=\"\"):\n",
    "    image = img['image']\n",
    "\n",
    "    # Print the class\n",
    "    class_str = img_class(img, idx)\n",
    "    print(f\"Label: {class_str}\")\n",
    "    \n",
    "    # Display the image\n",
    "    if display:\n",
    "        plt.figure()\n",
    "        plt.title(title_apend + class_str)\n",
    "        # Handle batches correctly\n",
    "        if image.shape.ndims > 3:\n",
    "            plt.imshow(image.numpy().reshape(64, 64, 3))\n",
    "        else:\n",
    "            plt.imshow(image.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print the dataset types and info\n",
    "print(\"--- Train & Validation dataset info ---\")\n",
    "print(f\"Train: {ds_train}\")\n",
    "print(f\"Validation: {ds_val}\")\n",
    "# print(f\"Dataset Info: {ds_info}\") # Uncomment to print Dataset info\n",
    "\n",
    "print(\"\\n--- Show an example image ---\")\n",
    "for example in ds_val.take(1):\n",
    "    img_info(example)\n",
    "\n",
    "print(\"\\n Show some other examples\")\n",
    "tfds.show_examples(ds_val, ds_info, rows=3, cols=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Print and visualize three inputs from the validation set\n",
    "#     : Print the stroage data type\n",
    "#     : Print and note the dimensions of each image\n",
    "#     : Print the memory required to store each image\n",
    "\n",
    "# Sample Images\n",
    "sample_imgs = []\n",
    "for index, img_data in enumerate(ds_val.take(3)):\n",
    "    sample_imgs.append(img_data)\n",
    "    image, label, id, label_name = img_data[\"image\"], img_data[\"label\"], img_data[\"id\"], img_data[\"metadata\"]['label_name']\n",
    "\n",
    "    print(f'\\n--- Image {index} ---')\n",
    "    # TODO: Your Code Here\n",
    "    # See example usage: https://github.com/duweisu/tiny-imagenet-tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Export each of the three inputs to a binary file which will be used to load the images into C++ later\n",
    "# NOTE: First flatten the array (ex: 4D --> 1D). So 64*64*3 = 12288 element 1D array\n",
    "\n",
    "# Make a directory for our image data\n",
    "img_dir = os.path.abspath('img_data')\n",
    "pathlib.Path(img_dir).mkdir(exist_ok=True)\n",
    "\n",
    "# Create a metadata file\n",
    "metadata_file = open(os.path.join(img_dir, f'metadata.txt'), 'w')\n",
    "metadata_file.write(f'Number\\t\\tDims\\t\\tClass Data\\n')\n",
    "\n",
    "# Export each image\n",
    "for index, img_data in enumerate(sample_imgs):    \n",
    "    img_file = open(os.path.join(img_dir, f'image_{index}.bin'), 'wb')\n",
    "    \n",
    "    # TODO: Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Loading and Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load the model\n",
    "# Now we will load the H5 model! Please make sure the h5 model file is present in the below directory.\n",
    "# You can download this from the Canvas Page and place it in the same directory as this notebook.\n",
    "\n",
    "# model_path = os.path.abspath(\"\"/home/<NETID>/path/to/your/lab1/CNN_TinyImageNet.h5)\" # Uncomment this to use a non-relative path\n",
    "model_path = os.path.abspath(\"CNN_TinyImageNet.h5\")\n",
    "\n",
    "# TODO: Your Code Here\n",
    "#model ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running infrence on our model\n",
    "# We can run an infrence of our model by doing the following (we are doing batches of 1 here)\n",
    "for example in ds_train.batch(1).take(1):\n",
    "    img_info(example)\n",
    "    \n",
    "    # Make a prediction\n",
    "    pred = model.predict(tf.cast(example[\"image\"], tf.float32)/255.0)\n",
    "    # print(f'Raw 200 Class Weighted Prediction:\\n{pred}') # Uncomment to see the raw prediction\n",
    "    \n",
    "    # What is out best guess?\n",
    "    best_guess = tf.math.argmax(pred, axis=1).numpy() # Our output is 200 weighted value, we want the most likely\n",
    "    print(f'Best Guess [class index]: {class_names[best_guess[0]]} [{best_guess[0]}]')\n",
    "    print(f'Best Guess Confidence (percent / 1.0): {pred[0][best_guess]}')\n",
    "\n",
    "    # What are our top 15 guesses?\n",
    "    top_15 = tf.math.top_k(pred, k=15)\n",
    "    print(f'Top 15 Guesses (class index): {[f\"{class_names[idx]} [{idx}]\" for idx in top_15.indices[0]]}')\n",
    "    print(f'Top 15 Guesses Confidence (percent / 1.0): {top_15.values}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run infrence for our previous 3 sample images\n",
    "\n",
    "# TODO: Your Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate the Top-1, Top-5, and Top-10 Accuracy of the validation dataset\n",
    "total = acc_top1 = acc_top5 = acc_top10 = 0\n",
    "\n",
    "# TODO: Your Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Print all of the possible classes of the dataset\n",
    "\n",
    "# TODO: Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize the model in Netron (https://netron.app/) and include an image here.\n",
    "#tf.keras.utils.plot_model(model, \"model.png\", show_shapes=True, show_dtype=True, expand_nested=True) # Uncomment this to generate a simple visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can view the layer weights. Here we consider them as images of feature filters applied to intermediate feature map images.\n",
    "# TODO: Visualize the 2 convolutional layers filter sets (weights) (one at the beginning and one at the end)\n",
    "\n",
    "# TODO: Your Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can view the layer outputs as well. Here we consider them as images of the spatial location of features.\n",
    "# TODO: Visualize the 2 convolutional layers outputs (intermediate feature maps) (one at the beginning and one at the end)\n",
    "\n",
    "# TODO: Your Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Export the filters/weights se we can use them later\n",
    "# Make a directory for our image data\n",
    "model_dir = os.path.abspath('model_data')\n",
    "pathlib.Path(model_dir).mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "# Export each image\n",
    "conv_index = dense_index = 1 # layer index starts from one\n",
    "for layer_idx, layer in enumerate(model.layers):\n",
    "    if re.match(r'(conv|dense)', layer.name):\n",
    "        weight_file_name = os.path.join(model_dir, f'{layer.name}_weights.bin')\n",
    "        bias_file_name = os.path.join(model_dir, f'{layer.name}_bias.bin')\n",
    "    else: continue\n",
    "\n",
    "    assert layer.weights[0].name.endswith('kernel')\n",
    "    assert layer.weights[1].name.endswith('bias')\n",
    "        \n",
    "    # TODO: Your Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Export the intermediate layer outputs for each of the input for all of the layers\n",
    "img_dir = os.path.abspath('img_data')\n",
    "pathlib.Path(img_dir).mkdir(exist_ok=True)\n",
    "\n",
    "for img_idx, img in enumerate(sample_imgs):\n",
    "    file_dir = os.path.join(img_dir, f'test_input_{img_idx}')\n",
    "    pathlib.Path(file_dir).mkdir(exist_ok=True)\n",
    "    \n",
    "    # TODO: Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup for profiling\n",
    "tf.profiler.experimental.ProfilerOptions(\n",
    "    host_tracer_level=1, python_tracer_level=0, device_tracer_level=1\n",
    ")\n",
    "\n",
    "log_dir = os.path.abspath(os.path.join('log_data'))\n",
    "log_dir_run = os.path.abspath(os.path.join(log_dir, datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")))\n",
    "pathlib.Path(log_dir_run).mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "try:\n",
    "    tf.profiler.experimental.stop()\n",
    "except:\n",
    "    test = 2\n",
    "finally:\n",
    "    test = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Sample Profiling - Inference for a single image:\n",
    "\n",
    "# Perform the inference profiling:\n",
    "for example in ds_train.batch(1).take(1):\n",
    "    # Starts Profile logging\n",
    "    tf.profiler.experimental.start(log_dir_run)\n",
    "    # Actual inference\n",
    "    # TODO: Your Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension.\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Launch TensorBoard and navigate to the Profile tab to view performance profile. \n",
    "# *** Please note just execute this command once in a session and \n",
    "# then logs for subsequent runs would be auto detected in tensorboard- url: http://localhost:6006/\n",
    "print(log_dir)\n",
    "%tensorboard --logdir={log_dir_run} --port=6006\n",
    "\n",
    "# You can view the tensorboard in the browser url: http://localhost:6006/\n",
    "\n",
    "# Useful command line to have if tensorboard is misbehaving: kill $(ps -e | grep 'tensorboard' | awk '{print $1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Sample Profiling - Online Inference:\n",
    "\n",
    "# Vary this from 10, 100, 1000 to simulate multiple online inference\n",
    "loop_index = [10, 100, 1000]\n",
    "\n",
    "for idx in loop_index:\n",
    "    # Starts Profile logging\n",
    "    tf.profiler.experimental.start(log_dir_run)\n",
    "\n",
    "    # Actual online inference\n",
    "    # TODO: Your Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Sample Profiling - Batch Inference:\n",
    "\n",
    "# We would only perform batch inference for a subset of validation set i.e. 1000 images \n",
    "# using different batch sizes of 20, 40, 100, 200 \n",
    "\n",
    "# Decides the size of the batch. Try: 20, 40, 100, 200\n",
    "batch_size = [20, 40, 100, 200]\n",
    "\n",
    "for batch in batch_size:\n",
    "    # Starts Profile logging\n",
    "    tf.profiler.experimental.start(log_dir_run)\n",
    "\n",
    "    # Actual Batch inference\n",
    "    # TODO: Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup for model training\n",
    "from tensorflow.keras import Model, datasets\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, ZeroPadding2D,Convolution2D, Activation, Dropout \n",
    "\n",
    "train_dir = os.path.abspath(os.path.join('train_data', datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")))\n",
    "pathlib.Path(train_dir).mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Using early stopping to monitor validation accuracy\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        # Stop training when `val_loss` is no longer improving\n",
    "        monitor=\"val_loss\",\n",
    "        # \"no longer improving\" being defined as \"no better than 1e-2 less\"\n",
    "        min_delta=1e-2,\n",
    "        # \"no longer improving\" being further defined as \"for at least 2 epochs\"\n",
    "        patience=2,\n",
    "        verbose=1,\n",
    "    ),\n",
    "    tf.keras.callbacks.TensorBoard(log_dir=train_dir, histogram_freq=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic CNN model\n",
    "train_model = Sequential()\n",
    "\n",
    "# conv1\n",
    "train_model.add(Conv2D(32, (5, 5), input_shape=(64, 64, 3), activation='relu'))\n",
    "train_model.add(Conv2D(32, (5,5),activation='relu'))\n",
    "train_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "train_model.add(Conv2D(64, (3,3), activation='relu'))\n",
    "train_model.add(Conv2D(64, (3,3), activation='relu'))\n",
    "train_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "train_model.add(Conv2D(64, (3,3), activation='relu'))\n",
    "train_model.add(Conv2D(128, (3,3), activation='relu'))\n",
    "train_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "train_model.add(Flatten())\n",
    "\n",
    "# fc1\n",
    "train_model.add(Dense(256, activation='relu'))\n",
    "\n",
    "# fc2\n",
    "train_model.add(Dense(200, activation='softmax'))\n",
    "\n",
    "train_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# TODO: Consider looking at different optimizers and learning rate settings\n",
    "train_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Attempt to train your own model with different batch sizes\n",
    "# TODO: See how long this takes without a GPU on your VDI or 2050 Coover machines\n",
    "# TODO: THEN log in to your GPU VM, set ENABLE_GPU = False in the very first cell, and re-run all above cells\n",
    "# TODO: Make sure you have exported the LD_LIBRARY_PATH as the lab manual indicates\n",
    "\n",
    "def normalize_img(image, label):\n",
    "    return tf.cast(image, tf.float32) / 255., label\n",
    "\n",
    "def to_categorical(image, label):\n",
    "    label = tf.one_hot(tf.cast(label, tf.int32), 200)\n",
    "    return tf.cast(image, tf.float32), tf.cast(label, tf.int64)\n",
    "\n",
    "ds_re = tiny_imagenet_builder.as_dataset(as_supervised=True)\n",
    "ds_retrain, ds_reval = ds_re[\"train\"], ds_re[\"validation\"]\n",
    "\n",
    "ds_retrain = ds_retrain.cache().shuffle(1024)\n",
    "ds_reval = ds_reval.cache().shuffle(1024)\n",
    "\n",
    "ds_retrain = ds_retrain.map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "ds_reval = ds_reval.map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "ds_retrain = ds_retrain.map(to_categorical, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "ds_reval = ds_reval.map(to_categorical, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "epoch_size = 20\n",
    "\n",
    "\n",
    "for batch_size in [32, 64, 128]:\n",
    "    # Setup our batched datasets\n",
    "    # TODO: Your Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train your model with 3 different numbers of epochs\n",
    "batch_size = 32\n",
    "\n",
    "# Setup your datasets\n",
    "# TODO: Your Code Here\n",
    "\n",
    "for epoch_size in [3, 10, 100]:\n",
    "    # Run training\n",
    "    # TODO: Your Code Here\n",
    "\n",
    "    # Save the cnn model\n",
    "    train_model.save(os.path.join(log_dir, f'CNN_TinyImageNet_train_batch{batch_size}.h5'))\n",
    "\n",
    "    # TODO: Get the top-1 and top-5 of your newly trained model\n",
    "    # TODO: Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Above and Beyond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark our dataset to make sure loading our data isn't a bottleneck ... and because we can\n",
    "# (This can be skipped since it can take a bit and is't all that important)\n",
    "\n",
    "# tfds.benchmark(ds_train.batch(32), batch_size=32, num_iter=2**20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore new models to find a higher-accuracy model. Does the new model require more or less time?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
